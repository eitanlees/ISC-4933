\section{Introduction to Linear Systems}
%% Lecture 01
A linear system is a set of equations of the form 
%
\begin{equation*}
    \sum_{j=1}^n a_{ij} x_j  = b_i \qquad i=1, \ldots, m
\end{equation*}
%
We are typically given $a_{ij}$ and one of $x_j$ or $b_i$. 
Our job is to find the other. In matrix form, 
\begin{equation*}
    A \Vec{x} = \Vec{b}
\end{equation*}
where 
$A \in \mathbb{R}^{m \times n}$, 
$\Vec{x}\in \mathbb{R}^{n}$, 
$\Vec{b}\in \mathbb{R}^{m}$
%
\begin{equation*}
    A = 
    \begin{bmatrix}
    a_{11} & a_{12} & \ldots & a_{1n}\\
    a_{21} & a_{22} & \ldots & a_{2n}\\
    \vdots & \vdots  & \ddots & \vdots\\
    a_{m1} & a_{m2} & \ldots & a_{mn}\\
    \end{bmatrix}
    ,\quad
    \Vec{x}  = \begin{bmatrix}x_1\\\vdots\\x_n\end{bmatrix}
    ,\quad
    \Vec{b}  = \begin{bmatrix}b_1\\\vdots\\b_m\end{bmatrix}
\end{equation*}

Linear systems arise when we 
\begin{itemize}[label={--}]
    \item Discretize a linear differential equation or integral equation.
    \item Linearizing a nonlinear differential equation 
	    or a nonlinear integral equation.
    \item Interpolating data with basis functions 
	    such as polynomials, wavelets, Fourier expansions.
    \item Optimizing an objective function $f\left(\Vec{x}\right)$. 
    \item Statistical operations such as lienar regression. 
    \item Solve an inverse problem. 
    \item Data sciences.
\end{itemize}

With $A\in\mathbb{R}^{m\times n}$, we have 3 cases

\begin{center}
	\begin{enumerate}[1)]
		\item $m<n$
		\item $m>n$
		\item $m=n$
	\end{enumerate}
\end{center}

If $m<n$, we have fewer equations than unknowns. 
Such a system typically has infinitely many solutions, 
and such a system is said to be \emph{underdetermined}. 
It can also have no solutions. 

\underline{EX}:
\begin{align*}
    x_1 + x_2 + 2x_3 &= 0 & x_1 + x_2 - 2x_3 &= 0\\
    x_1 - 2x_2 + 3x_3 &= 1 & 2x_1 +2x_2 - 4x_3&=7\\
    \text{\textcolor{red}{infinitely many }}&\text{\textcolor{red}{solutions}}
    & \text{\textcolor{red}{no solutions}}
\end{align*}

Underdetermined problems arise, for example, in inverse problems. 
Here, there is typically some kind of field that is unknown, 
and we only have a few measurements. 
For example in seismic, the distribution of the formations underground 
are predicted from only a few available measurements. 

If we have $m>n$, we have more equations than unknowns. 
Such a system typically has no solution, 
and such a system is said to be \emph{overdetermined}. 
It can also have 1 solution or infinitely many solutions. 


\underline{EX}:
\begin{align*}
    x+y&=1 & x+y&=1 & x+y&=1 \\
    2x+2y&=2 & 2x+2y&=2 & x-y&=2 \\
    3x+3y&=3 & 3x+4y&=3 & 3x-y&=4 \\
    \text{\textcolor{red}{infinitely many }}&\text{\textcolor{red}{solutions}}
    & &\hspace{-3em}\text{\textcolor{red}{one solutions}}
    & &\hspace{-3em}\text{\textcolor{red}{no solutions}}
\end{align*}

Overdetermined problems arise, for example, in interpolation. 
Suppose we are given data points $(x_i, y_i),\, i=1, \ldots, m$ 
and we seek a linear function interpolating this data.  
Suppose the interpolant is $f(x) = c_0 + c_1 x$
%
\begin{align*}
    c_0 + c_1x_1 &= y_1 \\
    c_0 + c_1x_2 &= y_2 \\
    \vdots\\
    c_0+c_1x_m &= y_m
\end{align*}


\begin{center}
	\input{fig/intro-linear-interpolation.tex}
\end{center}

Let's focus on the case $m=n$ and suppose that
$A\Vec{x} = \Vec{b}$ has a unique solution for every $\Vec{b}$. 
The three main tasks to be done are 
%
\begin{enumerate}[1)]
    \item Compute $A\Vec{x}$ for some $\Vec{x}\in\mathbb{R}^n$
    \item Solve $A\Vec{x}=\Vec{b}$ for some $\Vec{b}\in\mathbb{R}^n$
    \item Decompose $A$ as 
    
	    \begin{tabular}{lp{6cm}}
         $A=LU$ & Lower-Upper Decomposition  \\
         $A=QR$ & where Q is orthogonal ($Q^T=Q^{-1}$) and R is upper triangular\\
         $A=U\Sigma V^T$ & Singular Value Decomposition 
	 			where $U, V$ are orthogonal and $\Sigma$ is diagonal\\
         $A=VDV^{-1}$ & Eigenvalue Decomposition\\
         $A=A_{(:, J)}X$ & Interpolative Decomposition
    \end{tabular}
\end{enumerate}

%% Lecture 02
\subsection{Operation Counts}
Performing each of these tasks requires some number of floating point operations (flops). 
For example to compute $A\Vec{x} = \Vec{y}$ we would do the following
%
\begin{algorithm}
    \begin{algorithmic} 
        \FOR{$i = 1, \ldots, n$}
            \STATE $y_i = 0$ 
            \FOR{$j = 1, \ldots, n$}
                \STATE $y_i = y_i + a_{ij}x_j$ 
            \ENDFOR
        \ENDFOR
    \end{algorithmic}
\end{algorithm}
%
For each $i$, computing $y_i$ requires $n$ additions and $n$ multiplications. 
This must be done for each $i=1, \ldots, n$
%
\begin{displayquote}
$\Rightarrow$ We require $n \cdot 2n = 2n^2$ flops
\end{displayquote}
%
We are interested in the trend of the number of flops. 
Therefore, the 2 doesn't really matter. 
We write the following
%
\begin{displayquote}
    $\Rightarrow$ The number of flops to compute $A\Vec{x}$ is $\bigO{n^2}$
\end{displayquote}
%
The way to interpret the significance is to ask 
how many more flops are required if $n$ is doubled.
For the problem of size $n$, we require $Cn^2$ operations. 
If $n$ is doubled, we require $C(2n)^2=4Cn^2$
%
\begin{displayquote}
$\Rightarrow$ The increase in the amount of flops is $\frac{4Cn^2}{Cn^2} = 4$
\end{displayquote}

As we might expect, 
solving $A\Vec{x} = \Vec{b}$ is more expensive than doing matrix-vector multiplication. 
Suppose we use Gaussian elimination. 
Then, solving $A\Vec{x}=\Vec{b}$ is equivalent to reducing $A$ to 
%
\begin{enumerate}
    \item Reduced row echelon form
    \item Row echelon form and use back substitution
\end{enumerate}
%
Let's find the complexity (number of flops/operations) 
to reduce $A$ to row echelon form. 
Suppose row swaps are not required
%
\begin{center}
    \begin{tabular}{p{6cm}p{6cm}}
\begin{equation*}
    A = 
    \begin{bmatrix}  
        a_{11} & a_{12} & a_{13} & \ldots & a_{1n} \\
        a_{21} & a_{22} & a_{23} & \ldots & a_{2n} \\
        a_{31} & a_{32} & a_{33} & \ldots & a_{3n} \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        a_{n1} & a_{n2} & a_{n3} & \ldots & a_{nn} \\
    \end{bmatrix}  
\end{equation*} 
               & 
        \vspace{1em}
Replace row 2 with
\begin{equation*}
    \frac{a_{21}}{a_{11}} \times (\text{row 1}) - (\text{row 2})
\end{equation*}
        This requires $\bigO{n}$ flops             \\
    \end{tabular}
\end{center}
        %
        %
        %
\begin{center}
    \begin{tabular}{p{6cm}p{6cm}}
\begin{equation*}
    A = 
    \begin{bmatrix}  
        a_{11} & a_{12} & a_{13} & \ldots & a_{1n} \\
        0      & a_{22} & a_{23} & \ldots & a_{2n} \\
        a_{31} & a_{32} & a_{33} & \ldots & a_{3n} \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        a_{n1} & a_{n2} & a_{n3} & \ldots & a_{nn} \\
    \end{bmatrix}  
\end{equation*}
               & 
        \vspace{1em}
We next replace row 3 with
\begin{equation*}
    \frac{a_{31}}{a_{11}} \times (\text{row 1}) - (\text{row 3})
\end{equation*}
This requires $\bigO{n}$ flops                     \\
    \end{tabular}
\end{center}
        %
        %
        %
\begin{center}
    \begin{tabular}{p{6cm}p{6cm}}
\begin{equation*}
    A = 
    \begin{bmatrix}  
        a_{11} & a_{12} & a_{13} & \ldots & a_{1n} \\
        0      & a_{22} & a_{23} & \ldots & a_{2n} \\
        0      & a_{32} & a_{33} & \ldots & a_{3n} \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        a_{n1} & a_{n2} & a_{n3} & \ldots & a_{nn} \\
    \end{bmatrix}  
\end{equation*}
               & 
        \vspace{1em}
Continuing in this manner for all rows,
        we require $\bigO{n}$ operations $n-1$ times
        \begin{displayquote}
            $\Rightarrow \bigO{n^2}$ operations 
        \end{displayquote}
                                                   \\
    \end{tabular}
\end{center}
        %
        %
        %
\begin{center}
    \begin{tabular}{p{6cm}p{6cm}}
\begin{equation*}
    A = 
    \begin{bmatrix}  
        a_{11} & a_{12} & a_{13} & \ldots & a_{1n} \\
        0      & a_{22} & a_{23} & \ldots & a_{2n} \\
        0      & a_{32} & a_{33} & \ldots & a_{3n} \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        0      & a_{n2} & a_{n3} & \ldots & a_{nn} \\
    \end{bmatrix}  
\end{equation*}
               & 
        \vspace{1em}
        Moving to the 2\textsuperscript{nd} column, we use $a_{22}$ as a pivot to make $a_{i2}=0$
        for $i=3, \ldots, n$
    \end{tabular}
\end{center}

The first step is to replace row 3 with 
%
\begin{equation*}
    \frac{a_{32}}{a_{22}} (\text{row 1})-(\text{row 3})
\end{equation*}
%
This requires $\bigO{n-1}$ operations.

Repeating for each row means we require 
$\bigO{(n-1)(n-2)}$ operations to make the second column 
to be in the row echelon form.  
Continuing for each column, we require

\hspace{1em}
\begin{tabular}{ll}
    $\bigO{n^2}$ & operations for column 1 \\
    $\bigO{(n-1)^2}$ & operations for column 2\\
    $\bigO{(n-2)^2}$ & operations for column 3\\
    $\qquad\vdots$&\\
    $\bigO{1^2}$ & operations for column $(n-1)$
\end{tabular}

$\Rightarrow$ The total cost of reducing $A$ to row echelon form is 
%
\begin{gather*}
    Cn^2 + C(n-1)^2 + C(n-2)^2 + \ldots + C2^2 + C1^2 = C \sum_{j=1}^n j^2\\
    = C \frac{n(n+1)(2n+1)}{6} = \bigO{n^3} \text{ flops}
\end{gather*}
%
Reducing $A$ further to reduce row echelon form 
requires another $\mathcal{O}(n^3)$ operations.
Alternatively, we can solve 
%
\begin{equation*}
    U\Vec{x} = \Vec{b} 
\end{equation*}
%
where $U$ is the row echelon from of $A$, and 
$\Vec{b}$ has also been updated by the Gaussian
elimination procedure.

%% Lecture 03
 
\begin{equation*}
    \begin{bmatrix}  
        u_{11} & u_{12} & u_{13} & \ldots & u_{1n} \\
        0      & u_{22} & u_{23} & \ldots & u_{2n} \\
        0      & 0      & u_{33} & \ldots & u_{3n} \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        0      & 0      & 0      & \ldots & u_{nn} \\
    \end{bmatrix}  
    \begin{bmatrix}x_1\\x_2\\x_3\\\vdots\\x_n\end{bmatrix}=
    \begin{bmatrix}b_1\\b_2\\b_3\\\vdots\\b_n\end{bmatrix}
\end{equation*}
%
\begin{equation*}
  x = \frac{b_n}{u_{nn}}
\end{equation*}
%  
\begin{align*}
  u_{(n-1)(n-1)} x_{n-1} + u_{(n-1)n} x_n = b_{n-1} \quad &\Rightarrow \quad
  x_{n-1} = \frac{b_{n-1}-u_{(n-1)n}x_n}{u_{(n-1)(n-1)}}\\&\vdots
\end{align*}
%
\begin{equation*}
  x_k  = \frac{b_k - \sum_{j=k+1}^n u_{kj}x_j}{u_{kk}} \qquad k=n, n-1, \ldots, 3, 2, 1
\end{equation*}
%
Solving for $x_k$ requires $\bigO{n-k}$ operations to compute the summation. 
Therefore, solving for $\Vec{x}$ requires
%
\begin{equation*}
	\bigO{1+2+3+\ldots+n} = \bigO{\frac{n(n+1)}{2}} = \bigO{n^2}
\end{equation*}
%
Therefore, it is more efficient to reduce $A$ to row echelon form and apply back
substitution than it is to reduce $A$ to reduced row echeoln form. However,
since reducing $A$ to row echelon form requires $\bigO{n^3}$ operations, the
overall cost of solving $A\Vec{x}=\Vec{b}$ is $\bigO{n^3}$.

\subsection{LU Decomposition}

An alternative approach to solve $A\Vec{x}=\Vec{b}$ is to decompose $A$ into factors, 
where each factor is more efficient to work with.  
For instance, suppose we decompose $A$ as
%
\begin{equation*}
A=LU
\end{equation*}
%
where $L$ is lower triangular and $U$ is upper triangular.
Then to solve $A\Vec{x}=\Vec{b}$, we can instead solve
%
\begin{equation*}
LU\Vec{x}=\Vec{b}
\end{equation*}
%
Letting $\Vec{y}=U\Vec{x}$, this is equivalent to solving
%
\begin{align*}
  L\Vec{y}&=\Vec{b}\\
  U\Vec{x}&=\Vec{y}
\end{align*}
%
Solving each of these linear systems requires $\bigO{n^2}$ flops using
backward/forward substitution. However, we need to find $L$ and $U$.
Unfortunately, the $LU$ decomposition uses Gaussian elimination which we know
requires $\bigO{n^3}$ operations.

One instance where it makes sense to use $LU$ is when solving
$A\Vec{x_k}=\Vec{b_k}$ for many right ahnd sides $\Vec{b_k}$. We can then do the
following
%
\begin{algorithm}
    \begin{algorithmic} 
      \STATE Find $L$, $U$ with $A=UL$ \hspace{1 cm} \textcolor{red}{$\bigO{n^3}$}
      \FOR{$k=1, 2, 3, \ldots$}
        \STATE Solve $L\Vec{y}=\Vec{b}$ \hspace{1 cm} \textcolor{red}{$\bigO{n^2}$}
        \STATE Solve $U\Vec{x}=\Vec{y}$ \hspace{1 cm} \textcolor{red}{$\bigO{n^2}$}
      \ENDFOR
    \end{algorithmic} 
\end{algorithm}
%
If there are $M$ right hand sides, this requires $\bigO{n^3+Mn^2}$ operations, 
while applying Gaussian elimination requires $\bigO{Mn^3}$ operations.  

Another way to solve $A\Vec{x}=\Vec{b}$ is to use Cramer's Rule.
Suppose $A\Vec{x}=\Vec{b}$. 
Then, the $i$\textsuperscript{th} entry of $\Vec{x}$ is 
%
\begin{equation*}
	x_i = \frac{\det(A_i)}{\det(A)}
\end{equation*}
%
where $A_I$ is the matrix $A$ with it's 
$i$\textsuperscript{th} column replaced with $\Vec{b}$. 
Therefore, to find $\Vec{x}$, we have to compute $n+1$ determinates. 
The method you learnt in linear algebra is called the Laplace expansion. 
For $n=3$, this is 
%
\begin{align*}
  \det
  \begin{bmatrix}
    a_{11} & a_{12} & a_{13} \\ 
    a_{21} & a_{22} & a_{23} \\ 
    a_{31} & a_{32} & a_{33} \\ 
  \end{bmatrix}
  =
  a_{11}
  \begin{vmatrix}
    a_{22} & a_{23} \\ 
    a_{32} & a_{33} \\ 
  \end{vmatrix}
  -
    a_{12}
  \begin{vmatrix}
    a_{21} & a_{23} \\ 
    a_{31} & a_{33} \\ 
  \end{vmatrix}
  +
    a_{13}
  \begin{vmatrix}
    a_{21} & a_{22} \\ 
    a_{31} & a_{32} \\ 
  \end{vmatrix}
\end{align*}
%
\begin{displayquote}
  $\Rightarrow$ We have to compute 3 $2\times2$ determinates. 
\end{displayquote}
%
If $n=4$, we have to compute 4 $3\times3$ determinates, or $4\times3=12$
$2\times2$ determinates. For an arbitrary $n$, we have to compute
\begin{align*}
  &n \qquad (n-1)\times(n-1) \quad \text{determinates} \\
  & \parallel\\
  & n(n-1) \qquad (n-2)\times(n-2) \quad \text{determinates}\\
  & \parallel \quad \vdots\\
  & n(n-1)(n-2) \ldots (3) \qquad 2\times2 \quad \text{determinates}
\end{align*}
%
\begin{displayquote}
$\Rightarrow$ We require $\bigO{n!}$ operations
\end{displayquote}
%
Fortunately, computing $\det(A)$ is not an NP-hard problem. 
We can instead factor $A$ as $A=LU$ and use the property of determinates.
%
\begin{gather*}
  \det(A) = \det(LU) = \det(L) \times \det(U)\\
  = \prod_{k=1}^n l_{kk} \times \prod_{k=1}^n u_{kk}
\end{gather*}

%% Lecture 04

\subsection{Spectral Decomposition}

Another task that requires a determinant is the eigenvalue/eigenvector decomposition.  
Given $A\in\mathbb{R}^{n\times m}$, an eigenvalue $\lambda$ and 
a coresponding eigenvector $\Vec{x} \neq 0 $ satisfies
%
\begin{equation*}
  A \Vec{x} = \lambda \Vec{x} \Rightarrow A\Vec{x}=\lambda I \Vec{x}
  \Rightarrow (A-\lambda I) \Vec{x} = \Vec{0}
\end{equation*}
Since $\Vec{x} \neq 0$, we require that $A-\lambda I$ is not invertible
%
\begin{equation*}
  \det(A-\lambda I) = 0
\end{equation*}
This is always a polynomial of degree $n$, $p(\lambda)$, and we require $p(\lambda)=0$. 
Suppose
%
\begin{align*}
  A &= 
  \begin{bmatrix}
    -2 & -4 & 2 \\
    -2 & 1 & 2 \\
    4 & 2 & 5
  \end{bmatrix} 
  \Rightarrow \det(A-\lambda I) = 
            \begin{vmatrix}
    -2-\lambda & -4 & 2 \\
    -2 & 1-\lambda & 2 \\
    4 & 2 & 5-\lambda
  \end{vmatrix} \\
    &\Rightarrow
      (-2 - \lambda)
      \begin{vmatrix}
        1-\lambda & 2 \\
        2 & 5-\lambda
      \end{vmatrix}
      + 4
      \begin{vmatrix}
        -2 & 2 \\
        4 & 5-\lambda
      \end{vmatrix}
      + 2
      \begin{vmatrix}
        -2 & 1-\lambda \\
        4 & 2
      \end{vmatrix} = 0 \\
    &\Rightarrow
      (-2 \lambda) (\lambda^2 - 6 \lambda + 1) + 4 (2\lambda-18) + 2(-4\lambda-8) = 0 \\
    &\Rightarrow -\lambda^3 + 4 \lambda^2 +27 \lambda - 90 =0 \\
    &\Rightarrow (\lambda-3)(\lambda+5)(\lambda -6) = 0 \\
    &\Rightarrow \text{The eigenvalues are } \lambda=3, -5, 6
\end{align*}

To find the eigenvectors, say the one correspoding to $\lambda=3$
%
\begin{equation*}
  (A-\lambda I)\Vec{x} = \Vec{0} \Rightarrow (A - 3I)\Vec{x} = \Vec{0}
\end{equation*}
%
\begin{equation*}
  \begin{bmatrix}
    -5 & -4 & 2 \\
    -2 & -2 & 2 \\
    4  & 2  & 2 \\
  \end{bmatrix}
  \rightarrow
  \begin{bmatrix}
    1  & 1  & -1 \\
    -5 & -4 & 2 \\
    2  & 1  & 1 \\
  \end{bmatrix}
  \rightarrow
  \begin{bmatrix}
    1 & 1  & -1 \\
    0 & 1  & -3 \\
    0 & -1 & 3 \\
  \end{bmatrix}
  \rightarrow
  \begin{bmatrix}
    1 & 1  & -1 \\
    0 & 1  & -3 \\
    0 & 0  & 0 \\
  \end{bmatrix}
\end{equation*}
%
$\Rightarrow x_3$ is free. Say $x_3=1$.
%
\begin{align*}
  x_2 &= 3x_3 \Rightarrow x_2=3\\
  x_1 &= -x_2 + x_3 = -3 + 1 = -2 \\
  & \Rightarrow \Vec{x} = \begin{bmatrix}-2\\3\\1\end{bmatrix}
\end{align*}

We can easily check that $A\Vec{x}=3\Vec{x}$

To find all eigenvalues we must
\begin{enumerate}[1)]
  \item Compute an $n\times n$ determinate
  \item Find all roots, which may be complex, of a degree $n$ polynomial
\end{enumerate}
%
To find all of the eigenvectors, we must perform $n$ linear solves, each of size
$n\times n$.  
Ignoring the root finding problem, this requires at least $n\cdot
\bigO{n^3}=\bigO{n^4}$ operations.  
To find the roots of $p(\lambda)$, we must use a iterative solver if $n\leq 5$ (thanks to Galois).
Therefore, we must use an interative method such as \emph{Newton's method}.

Every algorithm we've considered so far is what your Linear Algebra instructor
taught you, but they are all impractical for large $n$.

\subsection{Schur Compliment}
We often solve linear systems with a natural partitioning
%
\begin{equation*}
  \begin{bmatrix}
    A & B \\
    C & D
  \end{bmatrix}
  \begin{bmatrix}
    \Vec{x} \\
    \Vec{y}
  \end{bmatrix}
  = 
  \begin{bmatrix}
    \Vec{b}_x \\
    \Vec{b}_y
  \end{bmatrix}
\end{equation*}
%
where
$A\in\mathbb{R}^{n\times n}$,
$B\in\mathbb{R}^{n\times m}$,
$C\in\mathbb{R}^{m\times n}$,
$D\in\mathbb{R}^{m\times m}$, 
$\Vec{x}\in\mathbb{R}^n$,
$\Vec{y}\in\mathbb{R}^m$,
$\Vec{b}_x\in\mathbb{R}^n$,
$\Vec{b}_y\in\mathbb{R}^m$,

If $D$ is invertible, then the 2\textsuperscript{nd} row tells us
%
\begin{equation*}
C\Vec{x} + D \Vec{y} = \Vec{b}_y \Rightarrow \Vec{y} = D^{-1} (\vec{b}_y - C\Vec{x})
\end{equation*}
%
Substituting this expression into the 1\textsuperscript{st} equation
\begin{align*}
  A\Vec{x} + B \Vec{y} = \Vec{b}_x &\Rightarrow A\Vec{x} + BD^{-1} (\vec{b}_y - C\Vec{x}) = \Vec{b}_x\\
  &\Rightarrow (A-BD^{-1}C) \Vec{x} = \Vec{b}_x -BD^{-1} \Vec{b}_y
\end{align*}
%
The matrix $A-BD^{-1}C$ is called the \emph{Schur compliment} of the block $D$.

Similarly, if $A$ is invertible, then
\begin{align*}
  A\Vec{x} + B \Vec{y} = \Vec{b}_x &\Rightarrow \Vec{x} = A^{-1}(\Vec{b}_x - B \Vec{y} ) \\
  C\Vec{x} + D \Vec{y} = \Vec{b}_y &\Rightarrow CA^{-1}(\Vec{b}_x - B \Vec{y} ) + D \Vec{y} = \Vec{b}_y\\
  &\Rightarrow (D-CA^{-1}B)\Vec{y} = \Vec{b}_y - CA^{-1}\Vec{b}_x
\end{align*}
%
The matrix $D-CA^{-1}B$ is called the \emph{Schur compliment} of the block $A$.
%
%% Lecture 05
%
Assuming $D$ and its Schur compliment $(A-B^{-1}C)$ are invertible, we can solve
for $\Vec{x}$ by
%
\begin{enumerate}[1)]
\item Form $D^{-1}C$ and $D^{-1}\Vec{b}_y$\\
  \textcolor{red}{$(m+1)$ applications of $D^{-1}$}
\item Solve
  \begin{equation*}
(A-BD^{-1}C)\Vec{x} = \Vec{b}_x - BD^{-1}\Vec{b}_y = \Vec{b}
  \end{equation*}
  \textcolor{red}{$(n \times n)$ linear solve.}
\end{enumerate}
%
Since $D$ is $m\times m$ and $A-BD^{-1}C$ is $n \times n$, these are smaller
linear systems than the one to find $\Vec{x}$ and $\Vec{y}$ simultaneously.

\subsection{Woodbury and Sherman-Morrison Identities}
Consider the matrix equation
%
\begin{equation*}
\begin{bmatrix}
  A & U \\
  V & -C^{-1}
\end{bmatrix}
\begin{bmatrix}
  X\\
  Y
\end{bmatrix}
=
\begin{bmatrix}
  I\\
  0
\end{bmatrix}
\end{equation*}
%
where
$A\in\mathbb{R}^{n \times n}$,
$U\in\mathbb{R}^{n \times k}$,
$V\in\mathbb{R}^{k \times n}$,
$C\in\mathbb{R}^{k \times k}$,
$X\in\mathbb{R}^{n \times n}$,
$Y\in\mathbb{R}^{k \times n}$,
$I\in\mathbb{R}^{n \times n}$,
$0\in\mathbb{R}^{k \times n}$,
%
Typically $k \ll n$.

We can use the Schur complement to find $X$
%
\begin{equation*}
VX-C^{-1}Y=0 \Rightarrow Y=CVX
\end{equation*}
%
Using the first equation
\begin{align*}
AX + UY = I &\Rightarrow AX + UCVX = I\\
            &\Rightarrow (A+UCV)X = I \\
            &\Rightarrow X=(A+UCV)^{-1}
\end{align*}
%
That is, $X$ is the inverse of $A+UCV$.

Next, we compute the Schur complement of the $(1,1)$ block
\begin{equation*}
AX+UY=I \Rightarrow X=A^{-1}(I-UY)
\end{equation*}
%
Substituting into the second equation
%
\begin{align*}
  VX-C^{-1} Y = 0 &\Rightarrow VA^{-1}(I-UY)-C^{-1}Y=0 \\
                  &\Rightarrow \textcolor{ForestGreen}{\underbrace{
                  \color{black}(-VA^{-1}U-C^{-1})}_{
                      \substack{\text{Schur complement of} \\ \text{the (1,1) block}}
                  }}Y = -VA^{-1}\\
                  &\Rightarrow Y = (C^{-1} + VA^{-1}U)^{-1}VA^{-1}
\end{align*}
%
Substituting this $Y$ back into the first equation
%
\begin{align*}
AX&+U(C^{-1} + VA^{-1}U)^{-1}VA^{-1} = I\\
  &\Rightarrow AX = I - U(C^{-1} + VA^{-1}U)^{-1}VA^{-1}\\
  &\Rightarrow X = A^{-1} - A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1}\\
\end{align*}
Recalling that $X=(A+UCV)^{-1}$, we have 
%
\begin{equation*}
  (A+UCV)^{-1} = A^{-1} - A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1}
\end{equation*}
%
This is called the \emph{Woodbury} identity.

It is useful if
\begin{enumerate}[1)]
\item $k$ is sufficiently small
\item A fast way to apply $A^{-1}$.
\end{enumerate}

To apply the Woodbury identity, we require
%
\textcolor{ForestGreen}{
\begin{itemize}[label={}]
\item $k+1$ applications of $A^{-1}$ 
\item 2 $k \times k$ linear solves
\end{itemize}
}

This identity is useful since we often have a solver for $A\Vec{x}=\Vec{b}$, and
then we need to invert a low rank modification of $A$.

The most extreme case is if $k=1$. We have $\Vec{u}, \Vec{v} \in \mathbb{R}^{n \times 1}$.
We can assume $c=1$ since any other non-zero value for $c$ can be rolled into
$\Vec{u}$(or $\Vec{v}$)
\begin{align*}
  \Rightarrow(A+\Vec{u}\Vec{v}^T)^{-1} &= A^{-1} - A^{-1}\Vec{u}(\textcolor{ForestGreen}{ \underbrace{\color{black}1 + \Vec{v}^TA^{-1}\Vec{u}}_{\mathbb{R}}})^{-1}\Vec{v}^TA^{-1}\\
                                       &=A^{-1} - \frac{A^{-1}\Vec{u}\Vec{v}^TA^{-1}}{1 + \Vec{v}^TA^{-1}\Vec{u}}
\end{align*}


This is the \emph{Sherman-Morrison} identity. Sometimes it is called the
\emph{SMW} or \emph{Sherman-Morrison-Woodbury} identity.

